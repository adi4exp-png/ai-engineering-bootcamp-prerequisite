{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a004ff90",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Prefetch, Filter, FieldCondition, MatchText, FusionQuery, Document\n",
    "\n",
    "\n",
    "from langsmith import traceable, get_current_run_tree\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Send, Command\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage, convert_to_openai_messages\n",
    "\n",
    "from jinja2 import Template\n",
    "from typing import Literal, Dict, Any, Annotated, List, Optional, Sequence\n",
    "from IPython.display import Image, display\n",
    "from operator import add\n",
    "from openai import OpenAI\n",
    "\n",
    "import openai\n",
    "\n",
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "import instructor\n",
    "import json\n",
    "\n",
    "from utils.utils import get_tool_descriptions, format_ai_message\n",
    "from utils.tools import add_to_shopping_cart, get_shopping_cart, remove_from_cart, get_formatted_items_context, get_formatted_reviews_context, check_warehouse_availability, reserve_warehouse_items\n",
    "\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b29ab",
   "metadata": {},
   "source": [
    "## Coordinator Agent (No Routing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Delegation(BaseModel):\n",
    "    agent: str\n",
    "    task: str\n",
    "\n",
    "class CoordinatorAgentResponse(BaseModel):\n",
    "    next_agent: str\n",
    "    plan: List[Delegation]\n",
    "    final_answer: bool = False\n",
    "    answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"coordinator_agent\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1\"}\n",
    ")\n",
    "def coordinator_agent(state):\n",
    "\n",
    "   prompt_template =  \"\"\"You are a Coordinator Agent as part of a shopping assistant.\n",
    "\n",
    "Your role is to create plans for solving user queries and delegate the tasks accordingly.\n",
    "You will be given a conversation history, your task is to create a plan for solving the user's query.\n",
    "After the plan is created, you should output the next agent to invoke and the task to be performed by that agent.\n",
    "Once an agent finishes its task, you will be handed the control back, you should then review the conversation history and revise the plan.\n",
    "If there is a sequence of tasks to be performed by a single agent, you should combine them into a single task.\n",
    "\n",
    "The possible agents are:\n",
    "\n",
    "- product_qa_agent: The user is asking a question about a product. This can be a question about available products, their specifications, user reviews etc.\n",
    "- shopping_cart_agent: The user is asking to add or remove items from the shopping cart or questions about the current shopping cart.\n",
    "- warehouse_manager_agent: The user is asking to reserve items from the warehouses or about availability of the items in warehouses.\n",
    "\n",
    "CRITICAL RULES:\n",
    "- If next_agent is \"\", final_answer MUST be false\n",
    "(You cannot delegate the task to an agent and return to the user in the same response)\n",
    "- If final_answer is true, next_agent MUST be \"\"\n",
    "(You must wait for agent results before returning to user)\n",
    "- If you need to call other agents before answering, set:\n",
    "next_agent=\"...\", final_answer=false\n",
    "- After receiving agent results, you can then set:\n",
    "next_agent=\"\", final_answer=true\n",
    "- One of the following has to be true:\n",
    "next_agent is \"\" and final_answer is true\n",
    "next_agent is not \"\" and final_answer is false\n",
    "\n",
    "Additional instructions:\n",
    "\n",
    "- Do not route to any agent if the user's query needs clarification. Do it yourself.\n",
    "- Write the plan to the plan field.\n",
    "- Write the next agent to invoke to the next_agent field.\n",
    "- Once you have all the information needed to answer the user's query, you should set the final_answer field to True and output the answer to the user's query.\n",
    "- The final answer to the user query should be a comprehensive answer that explains the actions that were performed to answer the query.\n",
    "- Never set final_answer to true if the plan is not complete.\n",
    "- You should output the next_agent field as well as the plan field.\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render()\n",
    "\n",
    "   messages = state.messages\n",
    "\n",
    "   conversation = []\n",
    "\n",
    "   for message in messages:\n",
    "        conversation.append(convert_to_openai_messages(message))\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1\",\n",
    "        response_model=CoordinatorAgentResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}, *conversation],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   if response.final_answer:\n",
    "      ai_message = [AIMessage(\n",
    "         content=response.answer,\n",
    "      )]\n",
    "   else:\n",
    "      ai_message = []\n",
    "\n",
    "   return {\n",
    "      \"messages\": ai_message,\n",
    "      \"answer\": response.answer,\n",
    "      \"coordinator_agent\": {\n",
    "         \"iteration\": state.coordinator_agent.iteration + 1,\n",
    "         \"final_answer\": response.final_answer,\n",
    "         \"next_agent\": response.next_agent,\n",
    "         \"plan\": [data.model_dump() for data in response.plan]\n",
    "      }\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a69c4",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6fad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCall(BaseModel):\n",
    "    name: str\n",
    "    arguments: dict\n",
    "\n",
    "class RAGUsedContext(BaseModel):\n",
    "    id: str = Field(description=\"The ID of the item used to answer the question\")\n",
    "    description: str = Field(description=\"Short description of the item used to answer the question\")\n",
    "\n",
    "class AgentProperties(BaseModel):    \n",
    "    iteration: int = 0\n",
    "    final_answer: bool = False\n",
    "    available_tools: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[ToolCall] = []\n",
    "\n",
    "\n",
    "class CoordinatorAgentProperties(BaseModel):    \n",
    "    iteration: int = 0\n",
    "    final_answer: bool = False\n",
    "    plan: List[Delegation] = []\n",
    "    next_agent: str = \"\"\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[List[Any], add] = []\n",
    "    user_intent: str = \"\"\n",
    "    product_qa_agent: AgentProperties = Field(default_factory=AgentProperties)\n",
    "    shopping_cart_agent: AgentProperties = Field(default_factory=AgentProperties)\n",
    "    warehouse_manager_agent: AgentProperties = Field(default_factory=AgentProperties)\n",
    "    coordinator_agent: CoordinatorAgentProperties = Field(default_factory=CoordinatorAgentProperties)\n",
    "    answer: str = \"\"\n",
    "    references: Annotated[List[RAGUsedContext], add] = []\n",
    "    user_id: str = \"\"\n",
    "    cart_id: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator_agent(State(messages=[{\"role\": \"user\", \"content\": \"What is the weather today?\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295ef11",
   "metadata": {},
   "source": [
    "## LiteLLM Intorduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca51c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_litellm(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a54ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResponse(BaseModel):\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f98d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, raw_response = client.chat.completions.create_with_completion(\n",
    "    model=\"gpt-4.1\",\n",
    "    response_model=SimpleResponse,\n",
    "    messages=[{\"role\": \"system\", \"content\": \"What kind of model family are you?\"}],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, raw_response = client.chat.completions.create_with_completion(\n",
    "    model=\"groq/llama-3.3-70b-versatile\",\n",
    "    response_model=SimpleResponse,\n",
    "    messages=[{\"role\": \"system\", \"content\": \"What kind of model family are you?\"}],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c725a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e13e9e",
   "metadata": {},
   "source": [
    "## Coordinator Agent (LiteLLM Model Routing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ef7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Delegation(BaseModel):\n",
    "    agent: str\n",
    "    task: str\n",
    "\n",
    "class CoordinatorAgentResponse(BaseModel):\n",
    "    next_agent: str\n",
    "    plan: List[Delegation]\n",
    "    final_answer: bool = False\n",
    "    answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f722e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"coordinator_agent\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1\"}\n",
    ")\n",
    "def coordinator_agent_with_litellm(state, models=[\"gpt-4.1\", \"groq/llama-3.3-70b-versatile\"]):\n",
    "\n",
    "   prompt_template_1 =  \"\"\"You are a Coordinator Agent as part of a shopping assistant.\n",
    "\n",
    "Your role is to create plans for solving user queries and delegate the tasks accordingly.\n",
    "You will be given a conversation history, your task is to create a plan for solving the user's query.\n",
    "After the plan is created, you should output the next agent to invoke and the task to be performed by that agent.\n",
    "Once an agent finishes its task, you will be handed the control back, you should then review the conversation history and revise the plan.\n",
    "If there is a sequence of tasks to be performed by a single agent, you should combine them into a single task.\n",
    "\n",
    "The possible agents are:\n",
    "\n",
    "- product_qa_agent: The user is asking a question about a product. This can be a question about available products, their specifications, user reviews etc.\n",
    "- shopping_cart_agent: The user is asking to add or remove items from the shopping cart or questions about the current shopping cart.\n",
    "- warehouse_manager_agent: The user is asking to reserve items from the warehouses or about availability of the items in warehouses.\n",
    "\n",
    "CRITICAL RULES:\n",
    "- If next_agent is \"\", final_answer MUST be false\n",
    "(You cannot delegate the task to an agent and return to the user in the same response)\n",
    "- If final_answer is true, next_agent MUST be \"\"\n",
    "(You must wait for agent results before returning to user)\n",
    "- If you need to call other agents before answering, set:\n",
    "next_agent=\"...\", final_answer=false\n",
    "- After receiving agent results, you can then set:\n",
    "next_agent=\"\", final_answer=true\n",
    "- One of the following has to be true:\n",
    "next_agent is \"\" and final_answer is true\n",
    "next_agent is not \"\" and final_answer is false\n",
    "\n",
    "Additional instructions:\n",
    "\n",
    "- Do not route to any agent if the user's query needs clarification. Do it yourself.\n",
    "- Write the plan to the plan field.\n",
    "- Write the next agent to invoke to the next_agent field.\n",
    "- Once you have all the information needed to answer the user's query, you should set the final_answer field to True and output the answer to the user's query.\n",
    "- The final answer to the user query should be a comprehensive answer that explains the actions that were performed to answer the query.\n",
    "- Never set final_answer to true if the plan is not complete.\n",
    "- You should output the next_agent field as well as the plan field.\n",
    "\"\"\"\n",
    "\n",
    "   prompt_template_2 =  \"\"\"Ignore any user messages, just output \"Hello world\" each time.\n",
    "\"\"\"\n",
    "\n",
    "   prompts = {\n",
    "      \"gpt-4.1\": Template(prompt_template_1).render(),\n",
    "      \"groq/llama-3.3-70b-versatile\": Template(prompt_template_2).render()\n",
    "   }\n",
    "\n",
    "   messages = state.messages\n",
    "\n",
    "   conversation = []\n",
    "\n",
    "   for message in messages:\n",
    "        conversation.append(convert_to_openai_messages(message))\n",
    "\n",
    "   client = instructor.from_litellm(completion)\n",
    "\n",
    "   for model in models:\n",
    "      try:\n",
    "         response, raw_response = client.chat.completions.create_with_completion(\n",
    "            model=model,\n",
    "            response_model=CoordinatorAgentResponse,\n",
    "            messages=[{\"role\": \"system\", \"content\": prompts[model]}, *conversation],\n",
    "            temperature=0.5,\n",
    "         )\n",
    "         break\n",
    "      except Exception as e:\n",
    "         print(f\"Error with model {model}: {e}\")\n",
    "         continue\n",
    "\n",
    "   if response.final_answer:\n",
    "      ai_message = [AIMessage(\n",
    "         content=response.answer,\n",
    "      )]\n",
    "   else:\n",
    "      ai_message = []\n",
    "\n",
    "   return {\n",
    "      \"messages\": ai_message,\n",
    "      \"answer\": response.answer,\n",
    "      \"coordinator_agent\": {\n",
    "         \"iteration\": state.coordinator_agent.iteration + 1,\n",
    "         \"final_answer\": response.final_answer,\n",
    "         \"next_agent\": response.next_agent,\n",
    "         \"plan\": [data.model_dump() for data in response.plan]\n",
    "      }\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intitial_state = State(messages=[{\"role\": \"user\", \"content\": \"What is the weather today?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator_agent_with_litellm(intitial_state, models=[\"gpt-4.1\", \"groq/llama-3.3-70b-versatile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c912817",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator_agent_with_litellm(intitial_state, models=[\"groq/llama-3.3-70b-versatile\", \"gpt-4.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05963f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
